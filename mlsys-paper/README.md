## Paper

- [Efficient Parallelization Layouts for Large-Scale Distributed Model Training](https://openreview.net/pdf?id=Y0AHNkVDeu)
- [LIGHTSEQ: SEQUENCE LEVEL PARALLELISM FOR DISTRIBUTED TRAINING OF LONG CONTEXT TRANS-FORMERS](https://arxiv.org/pdf/2310.03294.pdf)
- [NEAR ZERO BUBBLE PIPELINE PARALLELISM](https://openreview.net/pdf?id=tuzTN0eIO5)
- [Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models](https://arxiv.org/pdf/2401.04658.pdf)

